{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd510dcf-85b2-4396-858c-a14a37960c76",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c415d9d-1111-46ed-bc2d-849c75ab962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# added\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae871a-b31b-4f06-bb88-0fd1f6660b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18003e8e-4227-4e98-98de-0861ba8dc6cc",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76c413-001b-475e-9d1c-6662d25d2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying missing values\n",
    "print(train.isnull().sum())\n",
    "(train.isnull().sum() > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c62d6",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6afcd-a059-4c16-81a5-ab3d4d750a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337356e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist(figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2d1f1",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.filter(regex='X') # Input : X Featrue\n",
    "y_train = train.filter(regex='Y') # Output : Y Feature\n",
    "\n",
    "X_test = test.filter(regex='X') # Input : X Featrue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2d9e0",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c08064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# sc_X = MinMaxScaler()\n",
    "# sc_y = MinMaxScaler()\n",
    "# X_train = sc_X.fit_transform(X_train)\n",
    "# y_train = sc_y.fit_transform(y_train)\n",
    "\n",
    "# 중앙값과 IQR 사용하여 아웃라이어의 영향 최소화\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "sc_X = RobustScaler()\n",
    "sc_y = RobustScaler()\n",
    "\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# # X = X.copy() # solving thrown error\n",
    "\n",
    "# X.loc[:] = sc_x.fit_transform(X.loc[:]) # X.loc[:, ~X.columns.isin(['X_04', 'X_23', 'X_47', 'X_48'])] = sc_x.fit_transform(X.loc[:, ~X.columns.isin(['X_04', 'X_23', 'X_47', 'X_48'])]) # excluding categorical columns\n",
    "# y.loc[:] = sc_y.fit_transform(y.loc[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb14510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying overlapping data\n",
    "X.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ee7d0",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321272b9",
   "metadata": {},
   "source": [
    "## Unifying the Training set to have a single dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trains = []\n",
    "for i in range(1, 15):\n",
    "    y_name = 'Y_' + str(i).zfill(2)\n",
    "    X_trains.append(pd.concat([X, y[y_name]], axis=1))\n",
    "\n",
    "X_trains[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2147b1",
   "metadata": {},
   "source": [
    "## Iterating through the loop to find the best output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55384084",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb369f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(1,15): # ignore 'ID'\n",
    "        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:15])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea309ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor #, RandomForestRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "from pycaret.regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = GradientBoostingRegressor(loss=lg_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5594ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = [] # predicted y's\n",
    "\n",
    "# # iterate through the loop to predict each dependent variable\n",
    "# for i in range(len(X_trains)):\n",
    "#     y_col = X_trains[i].columns[-1]\n",
    "#     print(\"Currently processing '{}' Independent Variable\".format(y_col))\n",
    "    \n",
    "#     model = setup(X_trains[i], target=y_col, normalize=True, normalize_method='minmax', fold=5, fold_shuffle=True, use_gpu=True, session_id=SEED) # reset all weights in memory and create a new model\n",
    "    \n",
    "#     # models() # show all possible models\n",
    "    \n",
    "#     best_model = compare_models(include = ['gbr'], n_select=1) # find the best model\n",
    "    \n",
    "#     print(\"Best Model:\", best_model)\n",
    "\n",
    "#     clf = create_model(estimator=best_model, fold=5, cross_validation=True) # create the model\n",
    "#     tuned_clf = tune_model(clf) # find the optimal parameters\n",
    "    \n",
    "#     print(\"Evaluating the model performance...\")\n",
    "    \n",
    "#     evaluate_model(tuned_clf, use_train_data=True)\n",
    "    \n",
    "#     # predict the test set\n",
    "#     test_x = pd.read_csv('./test.csv').drop(columns=['ID']) # loading the test set\n",
    "    \n",
    "#     # test_x.loc[:] = sc_x.transform(test_x.loc[:]) # feature scaling\n",
    "#     results.append(np.expand_dims(predict_model(tuned_clf, data=test_x).iloc[:,-1], axis=1)) # store the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807103a7",
   "metadata": {},
   "source": [
    "## ~~Evaluating the model~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bca73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lg_nrmse(gt, preds):\n",
    "#     # 각 Y Feature별 NRMSE 총합\n",
    "#     # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "#     all_nrmse = []\n",
    "#     for idx in range(1,15): # ignore 'ID'\n",
    "#         rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "#         nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "#         all_nrmse.append(nrmse)\n",
    "#     score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:15])\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48680a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model(tuned_clf, use_train_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install shap\n",
    "# interpret_model(tuned_clf, plot='summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41cc9f-5cc4-4b03-9640-8c1982a7acad",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5e156-d7c4-400f-b552-0bb86ebfd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating the multiple results\n",
    "# preds = sc_y.inverse_transform(np.concatenate(results, axis=1))\n",
    "preds = np.concatenate(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b869ab6-6852-45a1-bc75-39dee40b583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, col in enumerate(submit.columns):\n",
    "    if col=='ID':\n",
    "        continue\n",
    "    submit[col] = preds[:,idx-1]\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b00cd-5680-4bf2-b7f8-0bea6ae8299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb0118d73d3d26f88e0d1e3500a56a7fc92e760d5128bb0bdb4d57f6bad5166f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
